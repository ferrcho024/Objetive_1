{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from funciones_OD import *\n",
    "\n",
    "ruta = \"G:/PhD/Datos SIATA/Análisis/Descriptivo/\"\n",
    "archivo = \"DF\"\n",
    "datos = pd.read_csv(ruta+\"set_datos_df/set_test_outliers.csv\",sep=\",\")\n",
    "entreno = pd.read_csv(ruta+\"set_datos_df/set_entrenamiento.csv\",sep=\",\")\n",
    "sensor = \"pm25\"\n",
    "redondeo = 4\n",
    "variables = ['pm25']#,'humedad_relativa', 'temperatura'\n",
    "for sensor in variables:\n",
    "    datos[sensor] = pd.Series([round(val,redondeo) for val in datos[sensor]])\n",
    "    entreno[sensor] = pd.Series([round(val,redondeo) for val in entreno[sensor]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAMIENTO DE HBOS\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "#************** PARA DATAFRAME MEZCLADO **********************\n",
    "# Solo descomentar las dos siguientes líneas\n",
    "#porcentaje = 0.7\n",
    "#entreno, test = df_mix (entreno,porcentaje)\n",
    "#*************************************************************\n",
    "\n",
    "#nodos_entreno = [50, 134, 187, 266]\n",
    "nodos_entreno = entreno.codigoSerial.unique().tolist()\n",
    "entrenamiento = pd.DataFrame()\n",
    "prediccion = pd.DataFrame()\n",
    "\n",
    "now = datetime.now()\n",
    "time_hbos = datetime(now.year, now.month, now.day, 00, 00, 00, 00000)\n",
    "time_percentil = datetime(now.year, now.month, now.day, 00, 00, 00, 00000)\n",
    "time_autoencoder = datetime(now.year, now.month, now.day, 00, 00, 00, 00000)\n",
    "\n",
    "# Beginning to prepare training and test datasets\n",
    "for i in nodos_entreno:\n",
    "    print(\"Nodo:\",i, \"Faltan:\", len(nodos_entreno)-(nodos_entreno.index(i)+1))\n",
    "    fild = entreno.loc[entreno.loc[:,\"codigoSerial\"] == i]\n",
    "    fild.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    if (len(fild)<=0):\n",
    "        print(\"No hay datos del nodo\", i)\n",
    "        continue\n",
    "        \n",
    "    diferencias(fild,variables,redondeo) # Calcula la diferencia entre dos valores contiguos\n",
    "\n",
    "    entrenamiento = pd.concat([entrenamiento,fild],ignore_index=True)\n",
    "\n",
    "entrenamiento[\"fechaHora\"] = entrenamiento[\"fecha\"] + \" \" + entrenamiento[\"hora\"]\n",
    "df_small_noise = entrenamiento.loc[:,[\"fechaHora\", variables[0]]]\n",
    "df_small_noise.set_index(\"fechaHora\", inplace=True)\n",
    "\n",
    "datos[\"fechaHora\"] = datos[\"fecha\"] + \" \" + datos[\"hora\"]\n",
    "df_daily_jumpsup = datos.loc[:,[\"fechaHora\", variables[0]]]\n",
    "pd.to_datetime(df_daily_jumpsup[\"fechaHora\"])\n",
    "df_daily_jumpsup.set_index(\"fechaHora\", inplace=True)\n",
    "\n",
    "# End to prepare training and test datasets \n",
    "\n",
    "# Calculating HBOS Score\n",
    "HBOS = {}\n",
    "hbos_inicio = datetime.now()\n",
    "\n",
    "ks,kd,n,tam_bin,bins = calcula_k(entrenamiento,variables,redondeo) # Calcula los intervalos de forma dinámica y estática y el tamaño del bin para cada uno\n",
    "\n",
    "for sensor in variables:\n",
    "    \n",
    "    # Asigna los valores a cada intervalo\n",
    "    intervalos = asigna_ks(entrenamiento[sensor+'_dif'],bins) # Asignación de valores en intervalos de forma estática\n",
    "    #intervalos = asigna_kd(n,entrenamiento[sensor],kd) # Asignación de valores en intervalos de forma dinpamica\n",
    "            \n",
    "    # Cálculo de ponderaciones de acuerdo con la frecuencia de cada intervalo (densidad de cada intervalo)\n",
    "    ponderaciones = calcula_ponderaciones(intervalos,redondeo)\n",
    "            \n",
    "    # Cálculo del scor HBOS para cada valor único.\n",
    "    HBOS = calcula_HBOS(ponderaciones,HBOS)\n",
    "    \n",
    "hbos_fin = datetime.now()\n",
    "time_hbos = time_hbos +(hbos_fin-hbos_inicio)\n",
    "\n",
    "# Fin to calculate HBOS score\n",
    "\n",
    "# Training Autoencoder\n",
    "\n",
    "autoencoder_inicio = datetime.now()\n",
    "training_mean = df_small_noise.mean()\n",
    "training_std = df_small_noise.std()\n",
    "df_training_value = (df_small_noise - training_mean) / training_std\n",
    "\n",
    "TIME_STEPS = 1\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "        layers.Conv1D(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot_convo.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    verbose=0,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")\n",
    "autoencoder_fin = datetime.now()\n",
    "time_autoencoder = time_autoencoder +(autoencoder_fin-autoencoder_inicio)\n",
    "# Fin to training autoencoder\n",
    "\n",
    "#### OUTLIER IDENTIFICATION ####\n",
    "\n",
    "\n",
    "datos2 = datos.copy()\n",
    "datos2=datos2.assign(HBOS=\"N\", Percetile=\"N\")\n",
    "ident_outliers = pd.DataFrame()\n",
    "nodos = datos2.codigoSerial.unique().tolist()\n",
    "fechas = datos2.fecha.unique().tolist()\n",
    "\n",
    "margen_HBOS = 4.5\n",
    "percentil = 90\n",
    "margen_percentil = 10\n",
    "\n",
    "cont = 0\n",
    "for i in nodos:\n",
    "    fild = datos2.loc[datos2.loc[:,\"codigoSerial\"] == i]\n",
    "    fild.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    diferencias(fild,variables,redondeo) # Calcula la diferencia entre dos valores contiguos\n",
    "\n",
    "    for j in fechas:\n",
    "       \n",
    "        fil = fild.loc[fild.loc[:,\"fecha\"] == j]\n",
    "        fil.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        if (len(fil)<=0):\n",
    "            continue\n",
    "\n",
    "        for sensor in variables:\n",
    "            percen = np.percentile(fil[sensor],[percentil])\n",
    "            #print(percen)\n",
    "    \n",
    "            for k in range(1,len(fil)):\n",
    "                cont += 1\n",
    "\n",
    "# HBOS Outlier Detection \n",
    "                hbos_inicio = datetime.now()         \n",
    "                # Validación de cada valor contra su respectivo score HBOS\n",
    "                hbos_inicio = datetime.now()\n",
    "                ponde = verifica_HBOS(HBOS,fil[variables[0]+'_dif'][k-1],bins)\n",
    "\n",
    "                if  ponde > margen_HBOS:\n",
    "                    ponde = verifica_HBOS(HBOS,fil[variables[0]+'_dif'][k],bins)\n",
    "                    if  ponde > margen_HBOS: \n",
    "                        fil.loc[k-1,\"HBOS\"]= \"S\"         \n",
    "                                    \n",
    "                hbos_fin = datetime.now()\n",
    "                time_hbos = time_hbos +(hbos_fin-hbos_inicio)\n",
    "                \n",
    "\n",
    "\n",
    "# Percentile Outlier Detection\n",
    "                percentil_inicio = datetime.now()\n",
    "                if (fil[sensor][k] > percen) and ((abs(fil[sensor][k] - fil[sensor][k-1])>margen_percentil) or (fil['Percentile'][k-1] == \"S\")):\n",
    "                    fil.loc[k,('Percentile')]= \"S\"\n",
    "                percentil_fin = datetime.now()\n",
    "                time_percentil = time_percentil +(percentil_fin-percentil_inicio)\n",
    "            \n",
    "        ident_outliers = pd.concat([ident_outliers,fil],ignore_index=True)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "# Autoencoder Outlier Detection\n",
    "autoencoder_inicio = datetime.now()\n",
    "x_train_pred = model.predict(x_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold = np.max(train_mae_loss)\n",
    "\n",
    "df_test_value = (df_daily_jumpsup - training_mean) / training_std\n",
    "\n",
    "# Create sequences from test values.\n",
    "x_test = create_sequences(df_test_value.values)\n",
    "\n",
    "# Get test MAE loss.\n",
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "# Detect all the samples which are anomalies.\n",
    "anomalies = test_mae_loss > threshold\n",
    "\n",
    "autoencoder_fin = datetime.now()\n",
    "time_autoencoder = time_autoencoder +(autoencoder_fin-autoencoder_inicio)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######  CONFUSION MATRIX #########\n",
    "Evaluacion_ident_outliers = pd.DataFrame(columns =[\"Tecnique\",\"Variable\",\"Precision\",\"Recall\",\"F1\",\"Accuracy\",\"Time\"])\n",
    "\n",
    "# Autoencoder\n",
    "lista_pred = []\n",
    "for i in range (len(x_test_pred)):\n",
    "    lista_pred.append(np.mean(x_test_pred[i]))\n",
    "    \n",
    "real = datos2.pm25_outlier.tolist()\n",
    "real = ['S' if value!='N' else value for value in real]\n",
    "predicho = ['S' if value==True else value for value in anomalies]\n",
    "predicho = ['N' if value==False else value for value in predicho]\n",
    "\n",
    "acc, re, pre, f1 = confu_matrix_no_print (real,predicho)\n",
    "\n",
    "Evaluacion_ident_outliers.loc[len(Evaluacion_ident_outliers)]=[\"Autoencoder\", TIME_STEPS, pre, re, f1, acc,time_autoencoder]\n",
    "\n",
    "# HBOS\n",
    "real = ident_outliers.pm25_ourlier.tolist()\n",
    "real = ['S' if value!='N' else value for value in real]\n",
    "predicho = ident_outliers.HBOS.tolist()\n",
    "\n",
    "acc, re, pre, f1 = confu_matrix_no_print (real,predicho)\n",
    "\n",
    "Evaluacion_ident_outliers.loc[len(Evaluacion_ident_outliers)]=[\"HBOS\", margen_HBOS, pre, re, f1, acc, time_hbos]\n",
    "\n",
    "# Percentil\n",
    "real = ident_outliers.pm25_outlier.tolist()\n",
    "real = ['S' if value!='N' else value for value in real]\n",
    "predicho = ident_outliers.Percentile.tolist()\n",
    "confu_matrix (real,predicho)\n",
    "\n",
    "acc, re, pre, f1 = confu_matrix_no_print (real,predicho)\n",
    "margen = str(percentil)+\"/\"+str(margen_percentil)\n",
    "\n",
    "Evaluacion_ident_outliers.loc[len(Evaluacion_ident_outliers)]=[\"Percentile\", margen, pre, re, f1, acc, time_percentil]\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

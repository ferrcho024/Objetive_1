{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haversine in c:\\users\\julio\\anaconda3\\lib\\site-packages (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sn\n",
    "import requests\n",
    "import json\n",
    "%pip install haversine\n",
    "import haversine as hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizen Scientist:  [1, 2, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 259, 261, 262, 265, 266, 267]\n",
      "Siata Stations:  [11, 12, 25, 28, 31, 37, 38, 44, 46, 48, 69, 6, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 92, 94]\n"
     ]
    }
   ],
   "source": [
    "#Read Data from February\n",
    "header_CC=[\"codigoSerial\", \"fecha\", \"hora\", \"fechaHora\", \"temperatura\", \"humedad_relativa\", \"pm1_df\", \"pm10_df\", \"pm25_df\", \"pm1_nova\", \"pm10_nova\", \"pm25_nova\", \"calidad_temperatura\", \"calidad_humedad_relativa\", \"calidad_pm1_df\", \"calidad_pm10_df\", \"calidad_pm25_df\", \"calidad_pm1_nova\", \"calidad_pm10_nova\", \"calidad_pm25_nova\"]\n",
    "datatypes_CC={\"codigoSerial\":np.uint16, \"temperatura\":np.float16, \"humedad_relativa\":np.float16, \"pm1_df\":np.float32, \"pm10_df\":np.float32, \"pm25_df\":np.float32, \"pm1_nova\":np.float32, \"pm10_nova\":np.float32, \"pm25_nova\":np.float32}\n",
    "df_CC = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/SIATA_CS/SplitDatosCC/Samples/\"+\"February.csv\", header=None, names=header_CC, usecols=header_CC , dtype=datatypes_CC,parse_dates=[\"fecha\",\"hora\",\"fechaHora\"])\n",
    "\n",
    "#Data includes January, February and March\n",
    "header_SS=[\"Fecha_Hora\",\"codigoSerial\",\"pm25\",\"calidad_pm25\",\"pm10\",\"calidad_pm10\"]\n",
    "datatypes_SS={\"codigoSerial\":np.uint16,\"pm25\":np.float32,\"pm10\":np.float32}\n",
    "df_SS = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/SIATA Stations/PM/\"+\"SS_PM.csv\", header=None,names=header_SS, usecols=header_SS , dtype=datatypes_SS,parse_dates=[\"Fecha_Hora\"])\n",
    "\n",
    "\n",
    "grouped=df_CC.groupby(df_CC.codigoSerial)\n",
    "CC={}\n",
    "print(\"Citizen Scientist: \", sorted(list(df_CC.codigoSerial.unique())))\n",
    "for i in df_CC.codigoSerial.unique():\n",
    "    CC[i] = grouped.get_group(i).sort_values(by=['fechaHora'],ignore_index=True)\n",
    "del df_CC\n",
    "\n",
    "grouped=df_SS.groupby(df_SS.codigoSerial)\n",
    "SS={}\n",
    "print(\"Siata Stations: \", list(df_SS.codigoSerial.unique()))\n",
    "for j in df_SS.codigoSerial.unique():\n",
    "    SS[j] = grouped.get_group(j).sort_values(by=['Fecha_Hora'],ignore_index=True)\n",
    "del df_SS\n",
    "del grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DISTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypesDistances={\"codigoSerial_CC\":np.uint16,\"codigoSerial_ES\":np.uint16,\"Distancia_a_ES\":np.float16}\n",
    "Distances = pd.read_csv(\"C:/Users/julio/Documents/UDEA/Maestría/DQ in IOT/Datasets/Distances and positions/Distancias_2.csv\", header=0, dtype=datatypesDistances,index_col=\"codigoSerial_CC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQ CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Nube: 220, Overall relative (Precision) Standard Deviation.\n",
      "1. Nube: 220, Overall relative Uncertainty BS, \n",
      "1. Nube: 220, Accuracy,\n",
      "1. Nube: 220, Completeness.\n",
      "1. Nube: 220, Overall concordance, \n",
      "2. Nube: 1, Overall relative (Precision) Standard Deviation.\n",
      "2. Nube: 1, Overall relative Uncertainty BS, \n",
      "2. Nube: 1, Accuracy,\n",
      "2. Nube: 1, Completeness.\n",
      "2. Nube: 1, Overall concordance, \n",
      "3. Nube: 192, Overall relative (Precision) Standard Deviation.\n",
      "3. Nube: 192, Overall relative Uncertainty BS, \n",
      "3. Nube: 192, Accuracy,\n",
      "3. Nube: 192, Completeness.\n",
      "3. Nube: 192, Overall concordance, \n",
      "4. Nube: 261, Overall relative (Precision) Standard Deviation.\n",
      "4. Nube: 261, Overall relative Uncertainty BS, \n",
      "4. Nube: 261, Accuracy,\n",
      "4. Nube: 261, Completeness.\n",
      "4. Nube: 261, Overall concordance, \n",
      "5. Nube: 262, Overall relative (Precision) Standard Deviation.\n",
      "5. Nube: 262, Overall relative Uncertainty BS, \n",
      "5. Nube: 262, Accuracy,\n",
      "5. Nube: 262, Completeness.\n",
      "5. Nube: 262, Overall concordance, \n",
      "6. Nube: 104, Overall relative (Precision) Standard Deviation.\n",
      "6. Nube: 104, Overall relative Uncertainty BS, \n",
      "6. Nube: 104, Accuracy,\n",
      "6. Nube: 104, Completeness.\n",
      "6. Nube: 104, Overall concordance, \n",
      "7. Nube: 143, Overall relative (Precision) Standard Deviation.\n",
      "7. Nube: 143, Overall relative Uncertainty BS, \n",
      "7. Nube: 143, Accuracy,\n",
      "7. Nube: 143, Completeness.\n",
      "7. Nube: 143, Overall concordance, \n",
      "8. Nube: 80, Overall relative (Precision) Standard Deviation.\n",
      "8. Nube: 80, Overall relative Uncertainty BS, \n",
      "8. Nube: 80, Accuracy,\n",
      "8. Nube: 80, Completeness.\n",
      "8. Nube: 80, Overall concordance, \n",
      "9. Nube: 169, Overall relative (Precision) Standard Deviation.\n",
      "9. Nube: 169, Overall relative Uncertainty BS, \n",
      "9. Nube: 169, Accuracy,\n",
      "9. Nube: 169, Completeness.\n",
      "9. Nube: 169, Overall concordance, \n",
      "10. Nube: 138, Overall relative (Precision) Standard Deviation.\n",
      "10. Nube: 138, Overall relative Uncertainty BS, \n",
      "10. Nube: 138, Accuracy,\n",
      "10. Nube: 138, Completeness.\n",
      "10. Nube: 138, Overall concordance, \n",
      "11. Nube: 140, Overall relative (Precision) Standard Deviation.\n",
      "11. Nube: 140, Overall relative Uncertainty BS, \n",
      "11. Nube: 140, Accuracy,\n",
      "11. Nube: 140, Completeness.\n",
      "11. Nube: 140, Overall concordance, \n",
      "12. Nube: 160, Overall relative (Precision) Standard Deviation.\n",
      "12. Nube: 160, Overall relative Uncertainty BS, \n",
      "12. Nube: 160, Accuracy,\n",
      "12. Nube: 160, Completeness.\n",
      "12. Nube: 160, Overall concordance, \n",
      "13. Nube: 142, Overall relative (Precision) Standard Deviation.\n",
      "13. Nube: 142, Overall relative Uncertainty BS, \n",
      "13. Nube: 142, Accuracy,\n",
      "13. Nube: 142, Completeness.\n",
      "13. Nube: 142, Overall concordance, \n",
      "14. Nube: 129, Overall relative (Precision) Standard Deviation.\n",
      "14. Nube: 129, Overall relative Uncertainty BS, \n",
      "14. Nube: 129, Accuracy,\n",
      "14. Nube: 129, Completeness.\n",
      "14. Nube: 129, Overall concordance, \n",
      "15. Nube: 34, Overall relative (Precision) Standard Deviation.\n",
      "15. Nube: 34, Overall relative Uncertainty BS, \n",
      "15. Nube: 34, Accuracy,\n",
      "15. Nube: 34, Completeness.\n",
      "15. Nube: 34, Overall concordance, \n",
      "16. Nube: 175, Overall relative (Precision) Standard Deviation.\n",
      "16. Nube: 175, Overall relative Uncertainty BS, \n",
      "16. Nube: 175, Accuracy,\n",
      "16. Nube: 175, Completeness.\n",
      "16. Nube: 175, Overall concordance, \n",
      "17. Nube: 89, Overall relative (Precision) Standard Deviation.\n",
      "17. Nube: 89, Overall relative Uncertainty BS, \n",
      "17. Nube: 89, Accuracy,\n",
      "17. Nube: 89, Completeness.\n",
      "17. Nube: 89, Overall concordance, \n",
      "18. Nube: 49, Overall relative (Precision) Standard Deviation.\n",
      "18. Nube: 49, Overall relative Uncertainty BS, \n",
      "18. Nube: 49, Completeness.\n",
      "18. Nube: 49, Overall concordance, \n",
      "19. Nube: 87, Overall relative (Precision) Standard Deviation.\n",
      "19. Nube: 87, Overall relative Uncertainty BS, \n",
      "19. Nube: 87, Accuracy,\n",
      "19. Nube: 87, Completeness.\n",
      "19. Nube: 87, Overall concordance, \n",
      "20. Nube: 240, Overall relative (Precision) Standard Deviation.\n",
      "20. Nube: 240, Overall relative Uncertainty BS, \n",
      "20. Nube: 240, Accuracy,\n",
      "20. Nube: 240, Completeness.\n",
      "20. Nube: 240, Overall concordance, \n",
      "21. Nube: 163, Overall relative (Precision) Standard Deviation.\n",
      "21. Nube: 163, Overall relative Uncertainty BS, \n",
      "21. Nube: 163, Accuracy,\n",
      "21. Nube: 163, Completeness.\n",
      "21. Nube: 163, Overall concordance, \n",
      "22. Nube: 18, Overall relative (Precision) Standard Deviation.\n",
      "22. Nube: 18, Overall relative Uncertainty BS, \n",
      "22. Nube: 18, Accuracy,\n",
      "22. Nube: 18, Completeness.\n",
      "22. Nube: 18, Overall concordance, \n",
      "23. Nube: 74, Overall relative (Precision) Standard Deviation.\n",
      "23. Nube: 74, Overall relative Uncertainty BS, \n",
      "23. Nube: 74, Accuracy,\n",
      "23. Nube: 74, Completeness.\n",
      "23. Nube: 74, Overall concordance, \n",
      "24. Nube: 37, Overall relative (Precision) Standard Deviation.\n",
      "24. Nube: 37, Overall relative Uncertainty BS, \n",
      "24. Nube: 37, Accuracy,\n",
      "24. Nube: 37, Completeness.\n",
      "24. Nube: 37, Overall concordance, \n",
      "25. Nube: 179, Overall relative (Precision) Standard Deviation.\n",
      "25. Nube: 179, Overall relative Uncertainty BS, \n",
      "25. Nube: 179, Accuracy,\n",
      "25. Nube: 179, Completeness.\n",
      "25. Nube: 179, Overall concordance, \n",
      "26. Nube: 234, Overall relative (Precision) Standard Deviation.\n",
      "26. Nube: 234, Overall relative Uncertainty BS, \n",
      "26. Nube: 234, Accuracy,\n",
      "26. Nube: 234, Completeness.\n",
      "26. Nube: 234, Overall concordance, \n",
      "27. Nube: 123, Overall relative (Precision) Standard Deviation.\n",
      "27. Nube: 123, Overall relative Uncertainty BS, \n",
      "27. Nube: 123, Accuracy,\n",
      "27. Nube: 123, Completeness.\n",
      "27. Nube: 123, Overall concordance, \n",
      "28. Nube: 204, Overall relative (Precision) Standard Deviation.\n",
      "28. Nube: 204, Overall relative Uncertainty BS, \n",
      "28. Nube: 204, Accuracy,\n",
      "28. Nube: 204, Completeness.\n",
      "28. Nube: 204, Overall concordance, \n",
      "29. Nube: 50, Overall relative (Precision) Standard Deviation.\n",
      "29. Nube: 50, Overall relative Uncertainty BS, \n",
      "29. Nube: 50, Accuracy,\n",
      "29. Nube: 50, Completeness.\n",
      "29. Nube: 50, Overall concordance, \n",
      "30. Nube: 141, Overall relative (Precision) Standard Deviation.\n",
      "30. Nube: 141, Overall relative Uncertainty BS, \n",
      "30. Nube: 141, Accuracy,\n",
      "30. Nube: 141, Completeness.\n",
      "30. Nube: 141, Overall concordance, \n",
      "31. Nube: 11, Overall relative (Precision) Standard Deviation.\n",
      "31. Nube: 11, Overall relative Uncertainty BS, \n",
      "31. Nube: 11, Accuracy,\n",
      "31. Nube: 11, Completeness.\n",
      "31. Nube: 11, Overall concordance, \n",
      "32. Nube: 19, Overall relative (Precision) Standard Deviation.\n",
      "32. Nube: 19, Overall relative Uncertainty BS, \n",
      "32. Nube: 19, Accuracy,\n",
      "32. Nube: 19, Completeness.\n",
      "32. Nube: 19, Overall concordance, \n",
      "33. Nube: 188, Overall relative (Precision) Standard Deviation.\n",
      "33. Nube: 188, Overall relative Uncertainty BS, \n"
     ]
    }
   ],
   "source": [
    "inicio=\"2020-02-01 00:00:00\"\n",
    "fin=   \"2020-02-29 23:59:00\"\n",
    "nube=0\n",
    "contador=0\n",
    "\n",
    "acc_vs_dis=[]\n",
    "missing_data_df=[]\n",
    "missing_data_nova=[]\n",
    "\n",
    "df_accur = pd.DataFrame(columns =['codigoSerial', 'dist', 'acc_nova', 'acc_df'])\n",
    "df_comple = pd.DataFrame(columns =[\"codigoSerial\",\"completeness_df\",\"completeness_nova\",\"completeness_group_df\",\"completeness_group_nova\"])\n",
    "df_preci = pd.DataFrame(columns =[\"codigoSerial\",\"precision_df\",\"precision_nova\",\"precision_group_df\",\"precision_group_nova\"])\n",
    "df_uncer = pd.DataFrame(columns =[\"codigoSerial\",\"uncertainty\",\"uncertainty_group\"])\n",
    "df_conco = pd.DataFrame(columns =[\"codigoSerial\",\"concordance_df_nova\",\"concordance_df_siata\",\"concordance_df_hum\",\"concordance_df_temp\",\"concordance_nova_siata\",\"concordance_nova_hum\",\"concordance_nova_temp\"])\n",
    "\n",
    "for nube in CC.keys():\n",
    "    contador+=1\n",
    "    CC[nube][\"v_pm25\"] = np.nan\n",
    "    CC[nube][\"alpha_df\"] = np.nan\n",
    "    CC[nube][\"alpha_nova\"] = np.nan\n",
    "    #del df_window\n",
    "    df_window=CC[nube][(CC[nube]['fechaHora'] >= inicio) & (CC[nube]['fechaHora'] <= fin)]\n",
    "    \n",
    "    #Remove outliers that are out of range, from documentation both nova and df range of measurements are [0,999]\n",
    "\n",
    "    df_window=df_window.copy()\n",
    "    df_window.loc[df_window[\"pm25_nova\"]>999,\"pm25_nova\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_nova\"]<0,\"pm25_nova\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_df\"]>999,\"pm25_df\"]=np.nan\n",
    "    df_window.loc[df_window[\"pm25_df\"]<0,\"pm25_df\"]=np.nan\n",
    "    \n",
    "    #Remove data above the whiskers of the boxplot\n",
    "    Q1 = df_window['pm25_df'].quantile(0.25)\n",
    "    Q3 = df_window['pm25_df'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_window.loc[df_window[\"pm25_df\"]>=Q3 + 1.5 *IQR,\"pm25_df\"]=np.nan\n",
    "    \n",
    "    Q1 = df_window['pm25_nova'].quantile(0.25)\n",
    "    Q3 = df_window['pm25_nova'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    df_window.loc[df_window[\"pm25_nova\"]>=Q3 + 1.5 *IQR,\"pm25_nova\"]=np.nan\n",
    "    \n",
    "    ref_date_range = pd.date_range(inicio, fin, freq='1Min')\n",
    "    ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\n",
    "    \n",
    "    #Hourly mean\n",
    "    df_window['pm25_nova_ave']=np.nan\n",
    "    df_window['pm25_df_ave']=np.nan\n",
    "    #Hourly standar deviation\n",
    "    df_window['pm25_nova_std']=np.nan\n",
    "    df_window['pm25_df_std']=np.nan\n",
    "    #Hourly uncertainty\n",
    "    df_window['pm25_unc']=np.nan\n",
    "    \n",
    "    for ts in df_window['fechaHora']:\n",
    "        if ts==ts.ceil('60min'):\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < (ts+timedelta(minutes = 1)).ceil('60min'))]\n",
    "            \n",
    "        else:\n",
    "            window=df_window[(df_window['fechaHora'] >= ts.floor('60min')) & (df_window['fechaHora'] < ts.ceil('60min'))]\n",
    "        #print(\"Timestamp: \",ts,\", Floor:\",ts.floor('60min'),\", Ceil:\",ts.ceil('60min'),window['pm25_nova'].mean())\n",
    "        #print(window['pm25_nova'])\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_ave']=window['pm25_nova'].mean()\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_ave']=window['pm25_df'].mean()\n",
    "        \n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_nova_std']=100*(window['pm25_nova'].std()/window['pm25_nova'].mean())\n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_df_std']=100*(window['pm25_df'].std()/window['pm25_df'].mean())\n",
    "        \n",
    "        df_window.loc[df_window[\"fechaHora\"]==ts,'pm25_unc']=\\\n",
    "        100*np.sqrt((window.pm25_df-window.pm25_nova).pow(2).mean()/2)/((window.pm25_df+window.pm25_nova).mean()/2)\n",
    "        \n",
    "    del window\n",
    "\n",
    "    prec_df=df_window.pm25_df_std.mean()\n",
    "    prec_nova=df_window.pm25_nova_std.mean()\n",
    "    uncer_df=df_window.pm25_unc.mean()\n",
    "    \n",
    "        \n",
    "    df_preci=df_preci.append({\"codigoSerial\":nube,\"precision_df\":prec_df,\"precision_nova\":prec_nova}, ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Overall relative (Precision) Standard Deviation.\"%(contador,nube))\n",
    "    \n",
    "    df_uncer=df_uncer.append({\"codigoSerial\":nube,\"uncertainty\":uncer_df}, ignore_index = True)\n",
    "    print(\"%d. Nube: %d, Overall relative Uncertainty BS, \"%(contador,nube))\n",
    "    \n",
    "    Closest_Station=Distances.codigoSerial_ES.loc[nube]    \n",
    "    if Closest_Station in SS.keys():\n",
    "        #Clean values out of range\n",
    "        SS[Closest_Station].loc[SS[Closest_Station][\"pm25\"]<=0,\"pm25\"]=np.nan\n",
    "        for time in df_window.fechaHora:\n",
    "            \n",
    "            idx=SS[Closest_Station].Fecha_Hora.searchsorted(time,side=\"right\")\n",
    "            #print(idx, SS[Closest_Station].Fecha_Hora.loc[idx], time)\n",
    "            v=SS[Closest_Station].loc[idx,\"pm25\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"v_pm25\"]=v\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_df_ave\"]\n",
    "            #print(time,\" : \",vm.values[0],\"________\",SS[Closest_Station].Fecha_Hora.loc[idx],\" : \",v)\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_df\"]=100*abs(vm-v)/v\n",
    "            vm=df_window.loc[(df_window.fechaHora == time),\"pm25_nova_ave\"]\n",
    "            df_window.loc[df_window.fechaHora == time,\"alpha_nova\"]=100*abs(vm-v)/v \n",
    "        \n",
    "        df_accur=df_accur.append({'codigoSerial':nube, 'dist':Distances.loc[nube,\"Distancia_a_ES\"], 'acc_df':df_window.alpha_df.mean(), 'acc_nova':df_window.alpha_nova.mean()}, ignore_index = True)\n",
    "        print(\"%d. Nube: %d, Accuracy,\" %(contador, nube))\n",
    "    \n",
    "    ref_date_range = pd.date_range(inicio, fin, freq='1Min')\n",
    "    ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\n",
    "\n",
    "    \n",
    "    #Check for any missing date\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\n",
    "\n",
    "    #Add missing date rows\n",
    "    for missing in missing_dates:\n",
    "        df_window=df_window.append({\"codigoSerial\":nube,\"fechaHora\":missing}, ignore_index = True)\n",
    "    \n",
    "    #Check for any missing date\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\n",
    "    \n",
    "    #Check for missing data\n",
    "    missing_data_df=np.count_nonzero(np.isnan(df_window['pm25_df']))\n",
    "    missing_data_nova=np.count_nonzero(np.isnan(df_window['pm25_nova']))\n",
    "    comp_df=100*(1-missing_data_df/np.size(df_window.pm25_df))\n",
    "    comp_nova=100*(1-missing_data_nova/np.size(df_window.pm25_nova))\n",
    "    \n",
    "    \n",
    "    if comp_df<75:\n",
    "        group_df=0\n",
    "    elif 75<=comp_df:\n",
    "        group_df=1\n",
    "    \n",
    "    if comp_nova<75:\n",
    "        group_nova=0\n",
    "    elif  75<=comp_nova:\n",
    "        group_nova=1     \n",
    "        \n",
    "    df_comple=df_comple.append({\"codigoSerial\":nube,\"completeness_df\":comp_df,\"completeness_nova\":comp_nova,\"completeness_group_df\":group_df,\"completeness_group_nova\":group_nova}, ignore_index = True)\n",
    "    \n",
    "    print(\"%d. Nube: %d, Completeness.\" %(contador,nube))\n",
    "    \n",
    "    corr_df = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[0].abs()\n",
    "    corr_nova = df_window.loc[:,[\"pm25_df\",\"pm25_nova\",\"v_pm25\",\"temperatura\",\"humedad_relativa\"]].corr().iloc[1].abs()\n",
    "  \n",
    "    \n",
    "        \n",
    "    df_conco=df_conco.append({\"codigoSerial\":nube,\"concordance_df_nova\":corr_df.pm25_nova,\n",
    "                              \"concordance_df_siata\":corr_df.v_pm25,\"concordance_df_hum\":corr_df.humedad_relativa,\"concordance_df_temp\":corr_df.temperatura,\n",
    "                              \"concordance_nova_siata\":corr_nova.v_pm25,\"concordance_nova_hum\":corr_nova.humedad_relativa,\"concordance_nova_temp\":corr_nova.temperatura}, \n",
    "                             ignore_index = True)\n",
    "    \n",
    "    print(\"%d. Nube: %d, Overall concordance, \"%(contador,nube))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\FERNAN~1\\AppData\\Local\\Temp/ipykernel_16380/924080128.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#Check for any missing date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmissing_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mref_date_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mref_date_range\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref_fechaHora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_window\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfechaHora\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ref_fechaHora\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#Add missing date rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_window' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "inicio=\"2020-02-01 00:00:00\"\n",
    "fin=   \"2020-02-29 23:59:00\"\n",
    "nube=0\n",
    "contador=0\n",
    "    \n",
    "for nube in [220]:\n",
    "    ref_date_range = pd.date_range(inicio, fin, freq='1Min')\n",
    "    ref_date_range = pd.DataFrame(ref_date_range,columns=[\"ref_fechaHora\"])\n",
    "\n",
    "    \n",
    "    #Check for any missing date\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\n",
    "\n",
    "    #Add missing date rows\n",
    "    for missing in missing_dates:\n",
    "        df_window=df_window.append({\"codigoSerial\":nube,\"fechaHora\":missing}, ignore_index = True)\n",
    "    \n",
    "    #Check for any missing date\n",
    "    missing_dates = ref_date_range.loc[~ref_date_range.ref_fechaHora.isin(df_window.fechaHora),\"ref_fechaHora\"]\n",
    "    \n",
    "    #Check for missing data\n",
    "    missing_data_df=np.count_nonzero(np.isnan(df_window['pm25_df']))\n",
    "    missing_data_nova=np.count_nonzero(np.isnan(df_window['pm25_nova']))\n",
    "    comp_df=100*(1-missing_data_df/np.size(df_window.pm25_df))\n",
    "    comp_nova=100*(1-missing_data_nova/np.size(df_window.pm25_nova))\n",
    "    \n",
    "    \n",
    "    if comp_df<75:\n",
    "        group_df=0\n",
    "    elif 75<=comp_df:\n",
    "        group_df=1\n",
    "    \n",
    "    if comp_nova<75:\n",
    "        group_nova=0\n",
    "    elif  75<=comp_nova:\n",
    "        group_nova=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_fechaHora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-01 00:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-01 00:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-01 00:06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-01 00:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20875</th>\n",
       "      <td>2020-02-29 23:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20876</th>\n",
       "      <td>2020-02-29 23:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20877</th>\n",
       "      <td>2020-02-29 23:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20878</th>\n",
       "      <td>2020-02-29 23:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20879</th>\n",
       "      <td>2020-02-29 23:58:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20880 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ref_fechaHora\n",
       "0     2020-02-01 00:00:00\n",
       "1     2020-02-01 00:02:00\n",
       "2     2020-02-01 00:04:00\n",
       "3     2020-02-01 00:06:00\n",
       "4     2020-02-01 00:08:00\n",
       "...                   ...\n",
       "20875 2020-02-29 23:50:00\n",
       "20876 2020-02-29 23:52:00\n",
       "20877 2020-02-29 23:54:00\n",
       "20878 2020-02-29 23:56:00\n",
       "20879 2020-02-29 23:58:00\n",
       "\n",
       "[20880 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_date_range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7563fc23a67347b817ff35caf6513d9bc5c54dd304f53e77ac1192a8a3e8c7a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
